{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "242f1dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "701db125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "builder. \\\n",
    "config('spark.ui.port', '0'). \\\n",
    "config('spark.shuffle.useOldFetchProtocol', 'true'). \\\n",
    "config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "enableHiveSupport(). \\\n",
    "master('yarn'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52353896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 items\n",
      "-rw-r--r--   2 hdfs supergroup   29485784 2021-01-28 11:30 /public/sms/users/users_01.json\n",
      "-rw-r--r--   2 hdfs supergroup   29609878 2021-01-28 09:18 /public/sms/users/users_02.json\n",
      "-rw-r--r--   2 hdfs supergroup   29614734 2021-01-28 09:07 /public/sms/users/users_03.json\n",
      "-rw-r--r--   2 hdfs supergroup   29598763 2021-01-28 09:00 /public/sms/users/users_04.json\n",
      "-rw-r--r--   2 hdfs supergroup   29614029 2021-01-28 11:13 /public/sms/users/users_05.json\n",
      "-rw-r--r--   2 hdfs supergroup   29600501 2021-01-28 11:03 /public/sms/users/users_06.json\n",
      "-rw-r--r--   2 hdfs supergroup   29597776 2021-01-28 11:01 /public/sms/users/users_07.json\n",
      "-rw-r--r--   2 hdfs supergroup   29601485 2021-01-28 08:51 /public/sms/users/users_08.json\n",
      "-rw-r--r--   2 hdfs supergroup   29589818 2021-01-28 11:19 /public/sms/users/users_09.json\n",
      "-rw-r--r--   2 hdfs supergroup   29600090 2021-01-28 09:20 /public/sms/users/users_10.json\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /public/sms/users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5ac6b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1328b5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"user_id\":1,\"user_first_name\":\"Lezley\",\"user_last_name\":\"D'Alessio\",\"user_email\":\"ldalessio0@google.com.au\",\"user_gender\":\"Male\",\"user_phone_numbers\":[\"5639521582\",\"8433335556\",\"9193704732\",\"8326122969\"],\"user_address\":{\"street\":\"28470 Di Loreto Point\",\"city\":\"Albany\",\"state\":\"New York\",\"postal_code\":\"12222\"}}\n",
      "{\"user_id\":2,\"user_first_name\":\"Boot\",\"user_last_name\":\"Cheetam\",\"user_email\":\"bcheetam1@alexa.com\",\"user_gender\":\"Male\",\"user_phone_numbers\":[\"9169982796\",\"3053640448\",\"3036850922\",\"2033582226\",\"5131711820\"],\"user_address\":{\"street\":\"035 Evergreen Place\",\"city\":\"Seattle\",\"state\":\"Washington\",\"postal_code\":\"98148\"}}\n",
      "{\"user_id\":3,\"user_first_name\":\"Natal\",\"user_last_name\":\"Cluff\",\"user_email\":\"ncluff2@de.vu\",\"user_gender\":\"Male\",\"user_phone_numbers\":[\"9172816696\",\"8012386892\",\"4053271574\",\"8125342052\"],\"user_address\":{\"street\":\"647 Lake View Circle\",\"city\":\"Des Moines\",\"state\":\"Iowa\",\"postal_code\":\"50936\"}}\n",
      "{\"user_id\":4,\"user_first_name\":\"Pedro\",\"user_last_name\":\"Riediger\",\"user_email\":\"priediger3@tiny."
     ]
    }
   ],
   "source": [
    "!hadoop fs -head /public/sms/users/users_01.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f6e79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g01.itversity.com:43785\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9dd4b2fba8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c18f07f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "users_schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"user_first_name\", StringType(), nullable=False),\n",
    "    StructField(\"user_last_name\", StringType(), nullable=False),\n",
    "    StructField(\"user_email\", StringType(), nullable=False),\n",
    "    StructField(\"user_gender\", StringType(), nullable=False),\n",
    "    StructField(\"user_phone_numbers\", ArrayType(StringType()), nullable=True),\n",
    "    StructField(\"user_address\", StructType([\n",
    "        StructField(\"street\", StringType(), nullable=False),\n",
    "        StructField(\"city\", StringType(), nullable=False),\n",
    "        StructField(\"state\", StringType(), nullable=False),\n",
    "        StructField(\"postal_code\", StringType(), nullable=False),\n",
    "    ]), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69ec105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.\\\n",
    "    format('json').\\\n",
    "    schema(users_schema).\\\n",
    "    load('/public/sms/users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aedde4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+-----------+--------------------+--------------------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|user_gender|  user_phone_numbers|        user_address|\n",
      "+-------+---------------+--------------+--------------------+-----------+--------------------+--------------------+\n",
      "| 200001|         Eirena|     Cutsforth|ecutsforth0@wisc.edu|     Female|[4197404036, 9173...|{8 Warrior Drive,...|\n",
      "| 200002|          Marja|      Shopcott|mshopcott1@hexun.com|     Female|[9542037028, 2128...|{66 Prairieview T...|\n",
      "| 200003|           Dawn|       Tointon|  dtointon2@ucsd.edu|     Female|[9523035647, 2134...|{18 Ronald Regan ...|\n",
      "| 200004|          Goldi|        Leaman|     gleaman3@360.cn|     Female|[2027069459, 7042...|{7696 Calypso Jun...|\n",
      "| 200005|       Brewster|      Hallagan|bhallagan4@livejo...|       Male|[8134746319, 2152...|{942 Emmet Park, ...|\n",
      "| 200006|       Florence|       Glashby|fglashby5@deviant...|     Female|[7571763565, 7134...|{664 Ridge Oak Ci...|\n",
      "| 200007|         Zollie|     Philimore|zphilimore6@webed...|       Male|[7865450177, 8019...|{4 Continental Dr...|\n",
      "| 200008|      Ferdinand|       Ramelot| framelot7@alexa.com|       Male|        [8164055187]|{44698 Arapahoe P...|\n",
      "| 200009|          Ruddy|     Greystoke|rgreystoke8@digg.com|       Male|                null|{null, null, null...|\n",
      "| 200010|         Gilles|         Beeho|   gbeeho9@webmd.com|       Male|[5599886694, 5201...|{9430 Reinke Park...|\n",
      "| 200011|           Axel|    Kitchinham|akitchinhama@nave...|       Male|        [2036913158]|{688 Granby Point...|\n",
      "| 200012|         Moreen|      Pearmine|mpearmineb@github...|     Female|        [4047148129]|{3997 Russell Par...|\n",
      "| 200013|       Joceline|       Adshead|jadsheadc@joomla.org|     Female|[3134837460, 3022...|{89 Fair Oaks Ter...|\n",
      "| 200014|          Roxie|       Menichi|rmenichid@nationa...|     Female|[6148474898, 9034...|{3 Green Pass, Fo...|\n",
      "| 200015|         Selena|         Flood|sfloode@delicious...|     Female|[7274706417, 4026...|{2 Butterfield Cr...|\n",
      "| 200016|         Leoine|      Markovic|  lmarkovicf@usa.gov|     Female|                null|{null, null, null...|\n",
      "| 200017|           Judi|      Svanetti|jsvanettig@theglo...|     Female|[2158389703, 8169...|{1 Sheridan Pass,...|\n",
      "| 200018|     Hildegarde|         Hapke|     hhapkeh@nps.gov|     Female|        [8183692525]|{60 Oak Valley St...|\n",
      "| 200019|          Hanni|       Dommett|hdommetti@tinyurl...|     Female|[7868201036, 7048...|{5 Vera Point, Wi...|\n",
      "| 200020|         Rupert|       Gaskoin|rgaskoinj@mozilla...|       Male|[5623065887, 9198...|{52636 Heath Alle...|\n",
      "+-------+---------------+--------------+--------------------+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f34a13d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf49cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total count of records in the Datarame\n",
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "187ec622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"users_9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14cc1536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>col_name</th><th>data_type</th><th>comment</th></tr>\n",
       "<tr><td>user_id</td><td>int</td><td>null</td></tr>\n",
       "<tr><td>user_first_name</td><td>string</td><td>null</td></tr>\n",
       "<tr><td>user_last_name</td><td>string</td><td>null</td></tr>\n",
       "<tr><td>user_email</td><td>string</td><td>null</td></tr>\n",
       "<tr><td>user_gender</td><td>string</td><td>null</td></tr>\n",
       "<tr><td>user_phone_numbers</td><td>array&lt;string&gt;</td><td>null</td></tr>\n",
       "<tr><td>user_address</td><td>struct&lt;street:str...</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------------+--------------------+-------+\n",
       "|          col_name|           data_type|comment|\n",
       "+------------------+--------------------+-------+\n",
       "|           user_id|                 int|   null|\n",
       "|   user_first_name|              string|   null|\n",
       "|    user_last_name|              string|   null|\n",
       "|        user_email|              string|   null|\n",
       "|       user_gender|              string|   null|\n",
       "|user_phone_numbers|       array<string>|   null|\n",
       "|      user_address|struct<street:str...|   null|\n",
       "+------------------+--------------------+-------+"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"describe extended users_9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11128b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>database</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td></td><td>users_9</td><td>true</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+---------+-----------+\n",
       "|database|tableName|isTemporary|\n",
       "+--------+---------+-----------+\n",
       "|        |  users_9|       true|\n",
       "+--------+---------+-----------+"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"show tables like 'users_9'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d113c1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7639dddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count(1)</th></tr>\n",
       "<tr><td>1000000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+\n",
       "|count(1)|\n",
       "+--------+\n",
       "| 1000000|\n",
       "+--------+"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from users_9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27a1bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting columns from nested json and creating views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f840a1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+-----------+--------------------+--------------------+--------------------+---------------+----------+-----------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|user_gender|  user_phone_numbers|        user_address|              street|           city|     state|postal_code|\n",
      "+-------+---------------+--------------+--------------------+-----------+--------------------+--------------------+--------------------+---------------+----------+-----------+\n",
      "| 200001|         Eirena|     Cutsforth|ecutsforth0@wisc.edu|     Female|[4197404036, 9173...|{8 Warrior Drive,...|     8 Warrior Drive|         Dallas|     Texas|      75358|\n",
      "| 200002|          Marja|      Shopcott|mshopcott1@hexun.com|     Female|[9542037028, 2128...|{66 Prairieview T...|66 Prairieview Te...|         Joliet|  Illinois|      60435|\n",
      "| 200003|           Dawn|       Tointon|  dtointon2@ucsd.edu|     Female|[9523035647, 2134...|{18 Ronald Regan ...|18 Ronald Regan Hill|Shawnee Mission|    Kansas|      66225|\n",
      "| 200004|          Goldi|        Leaman|     gleaman3@360.cn|     Female|[2027069459, 7042...|{7696 Calypso Jun...|7696 Calypso Junc...|     Saint Paul| Minnesota|      55166|\n",
      "| 200005|       Brewster|      Hallagan|bhallagan4@livejo...|       Male|[8134746319, 2152...|{942 Emmet Park, ...|      942 Emmet Park|    Albuquerque|New Mexico|      87110|\n",
      "+-------+---------------+--------------+--------------------+-----------+--------------------+--------------------+--------------------+---------------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,size\n",
    "df_add = df.withColumn(\"street\",df.user_address.street).\\\n",
    "            withColumn(\"city\",df.user_address.city).\\\n",
    "            withColumn(\"state\",df.user_address.state).\\\n",
    "            withColumn(\"postal_code\",df.user_address.postal_code)\n",
    "#df_loc = df[[user_id],[user_address]]\n",
    "df_add.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "639f18be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49576"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_add.filter(col('state') == 'New York').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f617beb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49576"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_add.filter(df_add.state == 'New York').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bca2b0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.createOrReplaceTempView(\"users_add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f0b92f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count(DISTINCT user_id)</th></tr>\n",
       "<tr><td>49576</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------------+\n",
       "|count(DISTINCT user_id)|\n",
       "+-----------------------+\n",
       "|                  49576|\n",
       "+-----------------------+"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct(user_id)) from users_add where state = 'New York'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3174c396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>state</th><th>cp</th></tr>\n",
       "<tr><td>California</td><td>206</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+---+\n",
       "|     state| cp|\n",
       "+----------+---+\n",
       "|California|206|\n",
       "+----------+---+"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select state,count(distinct(postal_code)) as cp from users_add group by state order by cp desc limit 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2d48ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|     state| cp|\n",
      "+----------+---+\n",
      "|California|206|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, countDistinct\n",
    "df_add.groupBy(\"state\").\\\n",
    "        agg(countDistinct(\"postal_code\").alias('cp')).\\\n",
    "        orderBy(\"cp\",ascending = False). \\\n",
    "        limit(1).show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "efdb0e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>city</th><th>cp</th></tr>\n",
       "<tr><td>Washington</td><td>28504</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-----+\n",
       "|      city|   cp|\n",
       "+----------+-----+\n",
       "|Washington|28504|\n",
       "+----------+-----+"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select city,count(distinct(user_id)) as cp from users_add where city is not null group by city order by cp desc limit 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57104045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      city|   cp|\n",
      "+----------+-----+\n",
      "|Washington|28504|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_add.filter(df_add['city'].isNotNull()).\\\n",
    "        groupBy(\"city\").\\\n",
    "        agg(countDistinct(\"user_id\").alias('cp')).\\\n",
    "        orderBy(\"cp\",ascending = False). \\\n",
    "        limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1266f628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>cp</th></tr>\n",
       "<tr><td>2015</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+\n",
       "|  cp|\n",
       "+----+\n",
       "|2015|\n",
       "+----+"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct(user_id)) as cp from users_add where user_email like '%biz'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f42cadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, countDistinct\n",
    "from pyspark.sql.functions import split\n",
    "df_ph_table = df_add.filter(split(df_add['user_email'],\"@\")[1] == 'bizjournals.com').\\\n",
    "        agg(countDistinct(\"user_email\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01795641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,size\n",
    "df_add.withColumn(\"no_of_phone_numbers\", size(col(\"user_phone_numbers\"))).createOrReplaceTempView(\"df_ph_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "668a79b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count(DISTINCT user_id)</th></tr>\n",
       "<tr><td>179041</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------------+\n",
       "|count(DISTINCT user_id)|\n",
       "+-----------------------+\n",
       "|                 179041|\n",
       "+-----------------------+"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct(user_id)) from df_ph_table where no_of_phone_numbers = 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed1a0ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count(user_email)</th></tr>\n",
       "<tr><td>2015</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+\n",
       "|count(user_email)|\n",
       "+-----------------+\n",
       "|             2015|\n",
       "+-----------------+"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ph_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84f4c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ph_table = df_add.withColumn(\"no_of_phone_numbers\", size(col(\"user_phone_numbers\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "98612ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|count(user_id)|\n",
      "+--------------+\n",
      "|        179041|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_ph_table.filter(col('no_of_phone_numbers') == 4).agg(countDistinct(\"user_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b076f5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count(DISTINCT user_id)</th></tr>\n",
       "<tr><td>108981</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------------+\n",
       "|count(DISTINCT user_id)|\n",
       "+-----------------------+\n",
       "|                 108981|\n",
       "+-----------------------+"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct(user_id)) from df_ph_table where user_phone_numbers is null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97245b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|count(user_id)|\n",
      "+--------------+\n",
      "|        108981|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_ph_table = df_add.withColumn(\"no_of_phone_numbers\", size(col(\"user_phone_numbers\")))\n",
    "df_ph_table.filter(col('user_phone_numbers').isNull()).agg(countDistinct(\"user_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6fa19fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user_id</th><th>user_first_name</th><th>user_last_name</th><th>user_email</th><th>user_gender</th><th>user_phone_numbers</th><th>user_address</th><th>street</th><th>city</th><th>state</th><th>postal_code</th><th>no_of_phone_numbers</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+---------------+--------------+----------+-----------+------------------+------------+------+----+-----+-----------+-------------------+\n",
       "|user_id|user_first_name|user_last_name|user_email|user_gender|user_phone_numbers|user_address|street|city|state|postal_code|no_of_phone_numbers|\n",
       "+-------+---------------+--------------+----------+-----------+------------------+------------+------+----+-----+-----------+-------------------+\n",
       "+-------+---------------+--------------+----------+-----------+------------------+------------+------+----+-----+-----------+-------------------+"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from df_ph_table where no_of_phone_numbers = 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "daf793a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+-----------+--------------------+--------------------+\n",
      "|user_id|user_first_name|user_last_name|          user_email|user_gender|  user_phone_numbers|        user_address|\n",
      "+-------+---------------+--------------+--------------------+-----------+--------------------+--------------------+\n",
      "| 200001|         Eirena|     Cutsforth|ecutsforth0@wisc.edu|     Female|[4197404036, 9173...|{8 Warrior Drive,...|\n",
      "| 200002|          Marja|      Shopcott|mshopcott1@hexun.com|     Female|[9542037028, 2128...|{66 Prairieview T...|\n",
      "| 200003|           Dawn|       Tointon|  dtointon2@ucsd.edu|     Female|[9523035647, 2134...|{18 Ronald Regan ...|\n",
      "| 200004|          Goldi|        Leaman|     gleaman3@360.cn|     Female|[2027069459, 7042...|{7696 Calypso Jun...|\n",
      "| 200005|       Brewster|      Hallagan|bhallagan4@livejo...|       Male|[8134746319, 2152...|{942 Emmet Park, ...|\n",
      "| 200006|       Florence|       Glashby|fglashby5@deviant...|     Female|[7571763565, 7134...|{664 Ridge Oak Ci...|\n",
      "| 200007|         Zollie|     Philimore|zphilimore6@webed...|       Male|[7865450177, 8019...|{4 Continental Dr...|\n",
      "| 200008|      Ferdinand|       Ramelot| framelot7@alexa.com|       Male|        [8164055187]|{44698 Arapahoe P...|\n",
      "| 200009|          Ruddy|     Greystoke|rgreystoke8@digg.com|       Male|                null|{null, null, null...|\n",
      "| 200010|         Gilles|         Beeho|   gbeeho9@webmd.com|       Male|[5599886694, 5201...|{9430 Reinke Park...|\n",
      "+-------+---------------+--------------+--------------------+-----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7ecf0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxr-xr-x   - itv011398 supergroup          0 2024-04-16 14:37 /user/itv011398/data/input\n",
      "drwxr-xr-x   - itv011398 supergroup          0 2024-03-13 19:23 /user/itv011398/data/output\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /user/itv011398/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d53f5c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.\\\n",
    "    mode(\"overwrite\").\\\n",
    "    format(\"parquet\"). \\\n",
    "    option(\"path\",\"/user/itv011398/data/wk_9\").\\\n",
    "    save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c77c0712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "-rw-r--r--   3 itv011398 supergroup          0 2024-10-08 17:04 /user/itv011398/data/wk_9/_SUCCESS\n",
      "-rw-r--r--   3 itv011398 supergroup   27109499 2024-10-08 17:04 /user/itv011398/data/wk_9/part-00000-cfdd27f9-88ec-4810-9905-a9280c1a32d3-c000.snappy.parquet\n",
      "-rw-r--r--   3 itv011398 supergroup   27112872 2024-10-08 17:04 /user/itv011398/data/wk_9/part-00001-cfdd27f9-88ec-4810-9905-a9280c1a32d3-c000.snappy.parquet\n",
      "-rw-r--r--   3 itv011398 supergroup   13778332 2024-10-08 17:04 /user/itv011398/data/wk_9/part-00002-cfdd27f9-88ec-4810-9905-a9280c1a32d3-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /user/itv011398/data/wk_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bd16f018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "963e7982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|count(user_email)|\n",
      "+-----------------+\n",
      "|             2015|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ph_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "653f6cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "###\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import col,size\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.shuffle.useOldFetchProtocol', 'true'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", \"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()\n",
    "###\n",
    "# creating schema for reading the json file\n",
    "'''\n",
    "users_schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"user_first_name\", StringType(), nullable=False),\n",
    "    StructField(\"user_last_name\", StringType(), nullable=False),\n",
    "    StructField(\"user_email\", StringType(), nullable=False),\n",
    "    StructField(\"user_gender\", StringType(), nullable=False),\n",
    "    StructField(\"user_phone_numbers\", ArrayType(StringType()), nullable=True),\n",
    "    StructField(\"user_address\", StructType([\n",
    "        StructField(\"street\", StringType(), nullable=False),\n",
    "        StructField(\"city\", StringType(), nullable=False),\n",
    "        StructField(\"state\", StringType(), nullable=False),\n",
    "        StructField(\"postal_code\", StringType(), nullable=False),\n",
    "    ]), nullable=False)\n",
    "])\n",
    "\n",
    "#reading the files\n",
    "users_df = spark.read \\\n",
    ".format(\"json\") \\\n",
    ".schema(users_schema) \\\n",
    ".load(\"/public/sms/users/\")\n",
    "\n",
    "users_df.withColumn(\"user_street\",col(\"user_address.street\")) \\\n",
    ".withColumn(\"user_city\",col(\"user_address.city\")) \\\n",
    ".withColumn(\"user_state\", col(\"user_address.state\")) \\\n",
    ".withColumn(\"user_postal_code\", col(\"user_address.postal_code\")) \\\n",
    ".withColumn(\"num_phn_numbers\", size(col(\"user_phone_numbers\"))).createOrReplaceTempView(\"users_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a2e9cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+\n",
      "|          user_state|Male_cnt|Female_cnt|\n",
      "+--------------------+--------+----------+\n",
      "|                Utah|    4073|      4171|\n",
      "|              Hawaii|    2172|      2062|\n",
      "|           Minnesota|    9371|      9250|\n",
      "|                Ohio|   16322|     16239|\n",
      "|              Oregon|    3899|      3841|\n",
      "|            Arkansas|    2420|      2416|\n",
      "|               Texas|   48786|     48450|\n",
      "|        North Dakota|     981|       940|\n",
      "|        Pennsylvania|   14270|     14237|\n",
      "|         Connecticut|    5797|      5917|\n",
      "|            Nebraska|    3501|      3688|\n",
      "|             Vermont|     227|       237|\n",
      "|              Nevada|    6317|      6495|\n",
      "|          Washington|    8812|      8755|\n",
      "|            Illinois|   11178|     11267|\n",
      "|            Oklahoma|    6888|      6913|\n",
      "|District of Columbia|   14212|     14292|\n",
      "|            Delaware|    1651|      1654|\n",
      "|              Alaska|    1882|      1938|\n",
      "|          New Mexico|    2804|      2745|\n",
      "+--------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT user_state,\n",
    "           COUNT(CASE WHEN user_gender = 'Male' THEN user_id END) AS Male_cnt,\n",
    "           COUNT(CASE WHEN user_gender = 'Female' THEN user_id END) AS Female_cnt\n",
    "    FROM users_vw\n",
    "    WHERE user_state IS NOT NULL \n",
    "          AND user_phone_numbers IS NOT NULL\n",
    "    GROUP BY user_state\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37d74548",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o322.save.\n: java.util.NoSuchElementException: None.get\n\tat scala.None$.get(Option.scala:529)\n\tat scala.None$.get(Option.scala:527)\n\tat org.apache.spark.sql.execution.datasources.BasicWriteJobStatsTracker$.metrics(BasicWriteStatsTracker.scala:175)\n\tat org.apache.spark.sql.execution.command.DataWritingCommand.metrics(DataWritingCommand.scala:51)\n\tat org.apache.spark.sql.execution.command.DataWritingCommand.metrics$(DataWritingCommand.scala:51)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.metrics$lzycompute(InsertIntoHadoopFsRelationCommand.scala:49)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.metrics(InsertIntoHadoopFsRelationCommand.scala:49)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.metrics$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.metrics(commands.scala:104)\n\tat org.apache.spark.sql.execution.SparkPlanInfo$.fromSparkPlan(SparkPlanInfo.scala:63)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:101)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-44076f037f37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \"\"\")\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0musers_final_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/user/itv011398/pivot_assignment_result\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o322.save.\n: java.util.NoSuchElementException: None.get\n\tat scala.None$.get(Option.scala:529)\n\tat scala.None$.get(Option.scala:527)\n\tat org.apache.spark.sql.execution.datasources.BasicWriteJobStatsTracker$.metrics(BasicWriteStatsTracker.scala:175)\n\tat org.apache.spark.sql.execution.command.DataWritingCommand.metrics(DataWritingCommand.scala:51)\n\tat org.apache.spark.sql.execution.command.DataWritingCommand.metrics$(DataWritingCommand.scala:51)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.metrics$lzycompute(InsertIntoHadoopFsRelationCommand.scala:49)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.metrics(InsertIntoHadoopFsRelationCommand.scala:49)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.metrics$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.metrics(commands.scala:104)\n\tat org.apache.spark.sql.execution.SparkPlanInfo$.fromSparkPlan(SparkPlanInfo.scala:63)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:101)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "#5 state-wise gender count\n",
    "users_final_df = spark.sql(\"\"\"\n",
    "    SELECT user_state,\n",
    "           COUNT(CASE WHEN user_gender = 'Male' THEN user_id END) AS Male_cnt,\n",
    "           COUNT(CASE WHEN user_gender = 'Female' THEN user_id END) AS Female_cnt\n",
    "    FROM users_vw\n",
    "    WHERE user_state IS NOT NULL \n",
    "          AND user_phone_numbers IS NOT NULL\n",
    "    GROUP BY user_state\n",
    "\"\"\")\n",
    "\n",
    "users_final_df.write.format(\"csv\").mode(\"overwrite\").option(\"path\",\"/user/itv011398/pivot_assignment_result\").save()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a46fb9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52 items\n",
      "-rw-r--r--   3 itv011398 supergroup          0 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/_SUCCESS\n",
      "-rw-r--r--   3 itv011398 supergroup         18 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00000-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         17 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00001-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         18 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00002-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         19 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00003-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         23 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00004-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         21 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00005-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         22 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00006-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         19 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00007-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         33 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00008-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         20 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00009-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         20 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00010-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         17 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00011-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         16 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00012-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         21 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00013-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         18 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00014-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         15 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00015-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         17 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00016-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         19 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00017-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         20 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00018-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         14 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00019-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         19 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00020-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         24 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00021-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         19 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00022-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         20 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00023-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         22 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00024-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         19 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00025-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         18 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00026-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         19 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00027-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         17 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00028-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         22 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00029-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         21 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00030-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         21 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00031-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         21 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00032-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         27 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00033-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         21 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00034-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         17 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00035-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         19 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00036-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         17 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00037-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         25 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00038-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         21 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00039-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         25 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00040-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         23 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00041-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         20 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00042-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         18 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00043-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         15 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00044-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         16 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00045-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         21 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00046-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         21 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00047-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         24 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00048-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         20 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00049-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n",
      "-rw-r--r--   3 itv011398 supergroup         16 2024-10-08 23:45 /user/itv011398/pivot_assignment_result/part-00050-35755110-9b26-43cf-9426-5791e7571ed6-c000.csv\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /user/itv011398/pivot_assignment_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf522322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alabama,9307,9178\n",
      "Alaska,1882,1938\n",
      "Arizona,9406,9543\n",
      "Arkansas,2420,2416\n",
      "California,49120,48716\n",
      "Colorado,10128,10125\n",
      "Connecticut,5797,5917\n",
      "Delaware,1651,1654\n",
      "District of Columbia,14212,14292\n",
      "Florida,36692,36688\n",
      "Georgia,13008,13028\n",
      "Hawaii,2172,2062\n",
      "Idaho,2058,2101\n",
      "Illinois,11178,11267\n",
      "Indiana,9604,9676\n",
      "Iowa,4706,4726\n",
      "Kansas,5962,5776\n",
      "Kentucky,6216,6108\n",
      "Louisiana,8706,8631\n",
      "Maine,225,228\n",
      "Maryland,5707,5797\n",
      "Massachusetts,6664,6610\n",
      "Michigan,8514,8625\n",
      "Minnesota,9371,9250\n",
      "Mississippi,2681,2599\n",
      "Missouri,9307,9547\n",
      "Montana,1225,1165\n",
      "Nebraska,3501,3688\n",
      "Nevada,6317,6495\n",
      "New Hampshire,754,722\n",
      "New Jersey,4268,4302\n",
      "New Mexico,2804,2745\n",
      "New York,25078,24498\n",
      "North Carolina,10896,10909\n",
      "North Dakota,981,940\n",
      "Ohio,16322,16239\n",
      "Oklahoma,6888,6913\n",
      "Oregon,3899,3841\n",
      "Pennsylvania,14270,14237\n",
      "Rhode Island,462,469\n",
      "South Carolina,4864,4793\n",
      "South Dakota,1206,1237\n",
      "Tennessee,8832,8813\n",
      "Texas,48786,48450\n",
      "Utah,4073,4171\n",
      "Vermont,227,237\n",
      "Virginia,15849,15607\n",
      "Washington,8812,8755\n",
      "West Virginia,4173,4281\n",
      "Wisconsin,4649,4744\n",
      "Wyoming,223,217\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat /user/itv011398/pivot_assignment_result/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a02997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
